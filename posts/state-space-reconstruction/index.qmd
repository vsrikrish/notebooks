---
title: "State-Space Reconstruction"
date: "2023-06-10"
categories: [math, stats]
date-modified: last-modified
---

Given a time series that we assume came from a dynamical system, can we reconstruct the state space of the system from the observations? This is complicated by a few features of chaotic or noisy systems.

## Setup and Theorem

Suppose we have a *deterministic* dynamical system with state $z(t)$ on a smooth manifold of dimension $m$. Our observations are $x(t) = g(z(t))$^[often $g(y) = y + \varepsilon_t$, where $\varepsilon_t$ is some noise process, but this can be more complex.].

For a time horizon $\tau < \infty$ and a positive integer $k$, set
$$s(t) = (x(t), x(t - \tau), x(t - 2\tau), \ldots, x(t - (k-1) \tau)).$$ It turns out [@Takens1981-gq] that if $k \geq 2m+1$, then $$z(t) = \phi(s(t))$$ for some diffeomorphism $\phi$.

A nice application of this is in the context of climate change. A common tendency along among certain climate-change skeptics^[Who know only enough to make these mistakes but not recognize them!] was to use Granger causality to claim that warming causes CO~2~ increases. This use violates the relevant assumptions of Granger causality, namely that there is no confounding causal influence on both time series^[In this case, Milankovitch cycles but we also know that the relationship between temperature and CO~2~ changes depending on the time scale of interest]. Instead, @Van_Nes2015-ua show using Takens' theorem that we get the expected feedbacks on the right time scales.

## Main Interest

My main interest is in getting this to work on multi-sector networks: can we reconstruct the state trajectory of a system of many state variables given a simulation? Using an ensemble of simulations would allow us to look for tipping points or teleconnections. The complications in making this tractable are stochasticity^[We would need to reconstruct a Markov process] and dimensionality^[Finding the relevant DAG is expensive.], but maybe looking at network properties can reduce the dimensionality. Maybe this can be done with the PC algorithm.

## Things I Need To Think About More

- Methods for reconstructing networks from potentially noisy simulations
- Use of network invariants for dimension-reduction. Maybe topological data analysis 

## Relevant Reading

### Reconstruction

- Takens., F. (1981). Detecting strange attractors in turbulence. In D. A. Rand & L. S. Young (Eds.), *Symposium on Dynamical Systems and Turbulence* (Springer Lecture Notes in Mathematics vol. 898) (pp. 366–381). Berlin, Germany: Springer.
- Stark, J., Broomhead, D. S., Davies, M. E., & Huke, J. (1997). Takens embedding theorems for forced and stochastic systems. *Nonlinear Analysis, Theory, Methods & Applications*, 30(8), 5303–5314. <https://doi.org/10.1016/S0362-546X(96)00149-6>
- Robinson, J. C. (2005). A topological delay embedding theorem for infinite-dimensional dynamical systems. *Nonlinearity*, 18(5), 2135. <https://doi.org/10.1088/0951-7715/18/5/013>
- Garcia, S. P., & Almeida, J. S. (2006, September 12). Multivariate phase space reconstruction by nearest neighbor embedding with different time delays. arXiv [nlin.CD]. Retrieved from <http://arxiv.org/abs/nlin/0609029>
- Spirtes, P., Glymour, C., & Scheines, R. (1993). Causation, Prediction, and Search. New York, NY: Springer. <https://doi.org/10.1007/978-1-4612-2748-9>
- Deyle, E. R., & Sugihara, G. (2011). Generalized theorems for nonlinear state space reconstruction. *PloS One*, 6(3), e18295. <https://doi.org/10.1371/journal.pone.0018295>
- Shalizi, C. R. (2003, May 12). Optimal Nonlinear Prediction of Random Fields on Networks. arXiv [math.PR]. Retrieved from <http://arxiv.org/abs/math/0305160>
- Shalizi, C. R., & Moore, C. (2003, March 29). What Is a Macrostate? Subjective Observations and Objective Dynamics. arXiv [cond-mat.stat-mech]. Retrieved from <http://arxiv.org/abs/cond-mat/0303625>
- Wingate, D., & Baveja, S. (2007). Exponential Family Predictive Representations of State. In J. Platt, D. Koller, Y. Singer, & S. Roweis (Eds.), Advances in Neural Information Processing Systems (Vol. 20). Retrieved from <https://proceedings.neurips.cc/paper_files/paper/2007/file/a9a1d5317a33ae8cef33961c34144f84-Paper.pdf>


### Algorithms

- Ye, H., Deyle, E. R., Gilarranz, L. J., & Sugihara, G. (2015). Distinguishing time-delayed causal interactions using convergent cross mapping. *Scientific Reports*, 5, 14750. <https://doi.org/10.1038/srep14750>
- Le, T. D., Hoang, T., Li, J., Liu, L., Liu, H., & Hu, S. (2019). A Fast PC Algorithm for High Dimensional Causal Discovery with Multi-Core PCs. *IEEE/ACM Transactions on Computational Biology and Bioinformatics / IEEE*, ACM, 16(5), 1483–1495. <https://doi.org/10.1109/TCBB.2016.2591526
- Langford, J., Salakhutdinov, R., & Zhang, T. (2009, May 20). Learning Nonlinear Dynamic Models. arXiv [cs.AI]. Retrieved from <http://arxiv.org/abs/0905.3369>
- Littman, M. L., Sutton, R. S., & Singh, S. (n.d.). Predictive Representations of State. In *NIPS 2001*. Retrieved from <https://web.eecs.umich.edu/~baveja/Papers/psr.pdf>
- Chalupka, K., Perona, P., & Eberhardt, F. (2014, December 7). Visual Causal Feature Learning. arXiv [stat.ML]. Retrieved from <http://arxiv.org/abs/1412.2309>
- Hefny, A., Downey, C., & Gordon, G. (2015, May 20). Supervised Learning for Dynamical System Learning. arXiv [stat.ML]. Retrieved from ,http://arxiv.org/abs/1505.05310>
- Subramanian, J., Sinha, A., Seraj, R., & Mahajan, A. (2022). Approximate Information State for Approximate Planning and Reinforcement Learning in Partially Observed Systems. Journal of Machine Learning Research: JMLR, 23(12), 1–83. Retrieved from <https://jmlr.org/papers/v23/20-1165.html>



 

